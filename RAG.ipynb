{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63cb400c",
   "metadata": {},
   "source": [
    "# RAG - Retrieval Augmented Generation\n",
    "\n",
    "The purpose of a RAG system is to generate a response that is relevant to the user's query. The system should be able to retrieve relevant information from a knowledge base and use it to generate a response. This notebook will demonstrate how to use the RAG model to generate responses to user queries.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "Before we start, make sure we have an embedding model to use for the RAG system.\n",
    "for this tutorial we will be using the `nomic-embed-text` model.\n",
    "\n",
    "in your terminal run the following command:\n",
    "\n",
    "```bash\n",
    "ollama pull nomic-embed-text\n",
    "```\n",
    "\n",
    "verify that the model is downloaded by running the following command:\n",
    "\n",
    "```bash\n",
    "ollama models\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dedde5",
   "metadata": {},
   "source": [
    "First we will need a document to work with. We will be using the a code of conduct as our document.\n",
    "Options for tools to load documents include:\n",
    "- `PyPDFLoader`\n",
    "- `WebBaseLoader`\n",
    "- `ObsidianLoader` \n",
    "- `RedditPostsLoader`\n",
    "- `RecursiveUrlLoader`\n",
    "- `TextLoader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e41434e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.\tCode of Conduct\n",
      "\n",
      "Our Code of Conduct outlines the fundamental principles and ethical standards th\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "urls = [\n",
    "    \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/6JDbUb_L3egv_eOkouY71A.txt\"\n",
    "]\n",
    "loader = WebBaseLoader(urls)\n",
    "documents = loader.load()\n",
    "print(documents[0].page_content[:100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829623bf",
   "metadata": {},
   "source": [
    "# Documents\n",
    "Loaders will return a list of documents. Each document is a dictionary with the following keys:\n",
    "- `page_content`: the text of the document\n",
    "- `metadata`: a dictionary with additional information about the document\n",
    "The document object will be the most important object in the RAG pipeline. It is used to create the embeddings of the documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fee750",
   "metadata": {},
   "source": [
    "# Chunking\n",
    "after loading in our documents, we can now chunk them into smaller documents. This is done by using the `chunk` function from the `langchain.text_splitter` module. We can specify the chunk size and the chunk overlap.\n",
    "\n",
    "notable splitters are:\n",
    "- `RecursiveCharacterTextSplitter`\n",
    "- `MarkdownTextSplitter`\n",
    "- `TextSplitter`\n",
    "- `CharacterTextSplitter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c33a1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accountability: We take responsibility for our actions and decisions. We follow all relevant laws and regulations, and we strive to continuously improve our practices. We report any potential violations of this code and support the investigation of such matters.\n",
      "Safety: We prioritize the safety of our employees, clients, and the communities we serve. We maintain a culture of safety, including reporting any unsafe conditions or practices.\n",
      "Environmental Responsibility: We are committed to minimizi\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.text_splitter import RecursiveCharacterTextSplitter\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "\n",
    "chunks = splitter.split_documents(documents=documents)\n",
    "print(chunks[2].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352cc6e2",
   "metadata": {},
   "source": [
    "# Vector Database\n",
    "with our documents chunked, we can now create a vector database. We will use the `ChromaDB` library for this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8e999a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Agentic AI Workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
