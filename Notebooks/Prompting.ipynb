{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3464cd64",
   "metadata": {},
   "source": [
    "# Prompting\n",
    "\n",
    "In this notebook, we will explore the concept of prompting in the context of LLMs. We will discuss the different types of prompts, how they work, and how they can be used to enhance the performance of LLMs.\n",
    "\n",
    "## Setting up the LLM using Ollama\n",
    "\n",
    "First, we need to install the Ollama software and set up the environment. You can find the installation instructions [here](https://ollama.ai/docs/installation).\n",
    "\n",
    "We will be using the `llama3.2:3b` model, which is a 32-bit version of the Llama model. This model is a good choice for beginners because it is lightweight and requires less computational resources.\n",
    "\n",
    "after verifying the installation of ollama, pull the model using the following command in the terminal:\n",
    "\n",
    "```bash\n",
    "ollama pull llama3.2:3b\n",
    "```\n",
    "\n",
    "you can verify the model is downloaded by running the following command:\n",
    "\n",
    "```bash\n",
    "ollama list\n",
    "```\n",
    "\n",
    "you can test run the model by running the following command:\n",
    "\n",
    "```bash\n",
    "ollama run llama3.2:3b\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d751f4e",
   "metadata": {},
   "source": [
    "With Ollama setup on our local machine, we can create interact with the model in python using the ollama library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7f1a4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "model_name = \"llama3.2:3b\"\n",
    "llm = ChatOllama(model=model_name)\n",
    "\n",
    "prompt = \"What is the capital of France?\"\n",
    "response = llm.invoke(prompt)\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fac2cf6",
   "metadata": {},
   "source": [
    "Using `PromptTemplate` from `langchain_core.prompts` we are able to create a prompt template that can be used to generate a prompt for a given input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01051744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the cat join a band?\n",
      "\n",
      "Because it wanted to be the purr-cussionist! (get it?)\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Tell me a {adjective} joke about {subject}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "formatted_prompt = prompt.format(adjective=\"funny\", subject=\"cats\")\n",
    "response = llm.invoke(formatted_prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d806740f",
   "metadata": {},
   "source": [
    "# Chain of Thought Prompting\n",
    "\n",
    "chain of thought prompting is a technique that allows the LLM to generate a response by breaking down a problem into multiple steps and generating a response for each step. This technique is useful when the LLM needs to reason about a complex problem and generate a sequence of actions to solve it.\n",
    "\n",
    "This can be helpful when debugging exactly how the LLM is generating a response, or when the LLM is struggling to understand a complex problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bb2a3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To find out how many apples are in the store now, we need to follow these steps:\n",
      "\n",
      "Step 1: Calculate the initial number of apples after selling some:\n",
      "The store had 22 apples initially.\n",
      "They sold 15 apples today.\n",
      "\n",
      "Subtracting the number of apples sold from the initial amount:\n",
      "22 - 15 = 7\n",
      "\n",
      "So, there are 7 apples left in the store before the new delivery.\n",
      "\n",
      "Step 2: Add the new delivery to the remaining apples:\n",
      "The store received a new delivery of 8 apples.\n",
      "Adding these new apples to the 7 apples already present:\n",
      "7 + 8 = 15\n",
      "\n",
      "Therefore, after the sale and the new delivery, there are now 15 apples in the store.\n"
     ]
    }
   ],
   "source": [
    "cot_template = \"\"\"\n",
    "Consider the problem: {problem}\n",
    "\n",
    "break down each step of your calculations\n",
    "\"\"\"\n",
    "cot_prompt = PromptTemplate.from_template(cot_template)\n",
    "\n",
    "cot_formatted_prompt = cot_prompt.format(\n",
    "    problem=\"A store had 22 apples. They sold 15 apples today and got a new delivery of 8 apples. How many apples are there now?\"\n",
    ")\n",
    "response = llm.invoke(cot_formatted_prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ef3079",
   "metadata": {},
   "source": [
    "# Few Shot Prompting\n",
    "Few shot prompting is a technique where we provide a few examples of the desired output and the model generates the rest of the output. This is useful when we want the model to generate a specific output for a given input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e649c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the statement \"I am so excited about my new job!\", the emotion can be classified as:\n",
      "\n",
      "Emotion: Euphoria\n",
      "\n",
      "Note that euphoria is a strong feeling of intense happiness or excitement, which aligns with the tone and language used in the statement.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Here are few examples of classifying emotions in statements:\n",
    "\n",
    "    Statement: 'I just won my first marathon!'\n",
    "    Emotion: Joy\n",
    "    \n",
    "    Statement: 'I can't believe I lost my keys again.'\n",
    "    Emotion: Frustration\n",
    "    \n",
    "    Statement: 'My best friend is moving to another country.'\n",
    "    Emotion: Sadness\n",
    "    \n",
    "    Now, classify the emotion in the following statement:\n",
    "    Statement: {statement}\n",
    "    Emotion:\n",
    "\"\"\"\n",
    "fs_prompt = PromptTemplate.from_template(prompt)\n",
    "formatted_fs_prompt = fs_prompt.format(statement=\"I am so excited about my new job!\")\n",
    "response = llm.invoke(formatted_fs_prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d00321",
   "metadata": {},
   "source": [
    "## Formatting outputs \n",
    "from the execution above, we can notice that the output is a longer string than we what expect. In that specific case we only wanted the emotion, so we can create a custom class that the llm will have to format its output to be in the format we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca039345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excitement\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class EmotionClassication(BaseModel):\n",
    "    emotion: str = Field(..., description=\"The emotion that the statement is associated with\")\n",
    "    \n",
    "structured_llm = llm.with_structured_output(EmotionClassication)\n",
    "response = structured_llm.invoke(formatted_fs_prompt)\n",
    "print(response.emotion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895b53e7",
   "metadata": {},
   "source": [
    "# Self-consistency Prompting\n",
    "self consistency prompting is a technique that allows the LLM to generate a response by comparing the generated response to a given input and generating a new response that is similar to the original response. This technique is useful when the LLM needs to generate a response that is consistent with the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6388be57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's calculate your sister's age using different methods.\n",
      "\n",
      "**Method 1: Using the initial condition**\n",
      "\n",
      "When you were 6 years old, your sister was half of your age. So, when you were 6, your sister's age was:\n",
      "\n",
      "6 / 2 = 3\n",
      "\n",
      "Now, let's add the number of years that have passed since then to find your sister's current age.\n",
      "\n",
      "You are now 70 years old, so let's calculate how many years have passed since you were 6:\n",
      "\n",
      "70 - 6 = 64 years\n",
      "\n",
      "Since your sister was 3 years old when you were 6, her current age would be:\n",
      "\n",
      "3 + 64 = 67 years old (for method 1)\n",
      "\n",
      "**Method 2: Using the ratio**\n",
      "\n",
      "When you were 6, your sister's age was half of yours. This means that for every year you age, your sister ages by half a year.\n",
      "\n",
      "Since you are now 70 years old, we can calculate how much you aged since then:\n",
      "\n",
      "70 - 6 = 64 years\n",
      "\n",
      "Now, let's multiply the number of years you aged by half a year to find your sister's additional aging during that time:\n",
      "\n",
      "64 x 0.5 = 32 years\n",
      "\n",
      "Since your sister was 3 years old when you were 6, we add the calculated age difference (32) to her initial age (3):\n",
      "\n",
      "3 + 32 = 35 years old (for method 2)\n",
      "\n",
      "**Method 3: Using algebra**\n",
      "\n",
      "Let's represent the number of years that have passed since you were 6 as x.\n",
      "\n",
      "We know that when you were 6, your sister was half of your age. So, if we let your current age be y and your sister's current age be z:\n",
      "\n",
      "y - 6 = x\n",
      "z - 3 = (x / 2)\n",
      "\n",
      "Since you are now 70 years old, we set up the equation for your current age:\n",
      "\n",
      "y - 6 = 64\n",
      "y = 64 + 6\n",
      "y = 70\n",
      "\n",
      "Substituting y into the second equation and solving for z:\n",
      "\n",
      "z - 3 = (64 / 2)\n",
      "z - 3 = 32\n",
      "z = 35 years old (for method 3)\n",
      "\n",
      "Now, let's compare the results from each calculation.\n",
      "\n",
      "**Consistency:**\n",
      "All three methods result in the same answer: your sister is 35 years old. This confirms that the initial condition and ratio used in Method 1 are consistent with this value, while Method 2 provides an alternative way to arrive at the same conclusion.\n"
     ]
    }
   ],
   "source": [
    "sc_template = \"\"\"\n",
    "    {question}\n",
    "    \n",
    "    Provide three independent calculations and explanations, then determine the most consistent result.\n",
    "\"\"\"\n",
    "sc_prompt = PromptTemplate.from_template(sc_template)\n",
    "formatted_sc_prompt = sc_prompt.format(question=\"When I was 6, my sister was half of my age. Now I am 70, what age is my sister?\")\n",
    "response = llm.invoke(formatted_sc_prompt)\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2263944a",
   "metadata": {},
   "source": [
    "# LCEL Chain. Piping the output of the LLM to another process.\n",
    "LCEL is a technique that allows the LLM to generate a response by breaking down a problem into multiple predefined steps and generating a response for each step. This technique is useful when you want to break down a complex problem into smaller, more manageable steps and generate a response for each step or if you need to format/process the output in a specific way.\n",
    "\n",
    "To start, lets create a simple chain that will take in a prompt, pass it to the LLM, and then return the output using `StrOutputParser`.\n",
    "\n",
    "`RunnableLambda` is a `Runnable` that will take our input and pass it through to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55df93c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Technology has had a profound impact on society in various ways. Some of the key effects include:\n",
      "\n",
      "1. Increased efficiency and productivity: Automation and robotics have improved manufacturing processes, allowing for faster production and higher quality goods.\n",
      "2. Job displacement and creation: While automation has replaced some jobs, it has also created new ones in fields such as AI development, maintenance, and management.\n",
      "3. Improved healthcare: Advanced medical technologies have enabled early disease detection, personalized medicine, and more effective treatments.\n",
      "4. Enhanced education: Online learning platforms and digital tools have democratized access to education, making it possible for people worldwide to acquire new skills and knowledge.\n",
      "5. Changes in communication: Technology has revolutionized the way we communicate, with social media, messaging apps, and video conferencing enabling global connectivity and real-time interaction.\n",
      "\n",
      "In the context of robotics and AI, technology is also transforming the way we interact with machines, allowing for more intuitive control and adaptability in dynamic environments. This shift towards intelligent, autonomous systems will likely continue to impact society in various ways, from improving public services to enhancing our daily lives.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "content = \"\"\"\n",
    "Robotics is shifting from basic automation toward intelligent, autonomous systems that can think and act in dynamic environments rather than follow simple pre-programmed instructions. \n",
    "This trend combines analytical AI and generative AI so robots can adapt in real time to new situations.\n",
    "This “Physical AI” approach builds robots that can learn from experience, generalize to different tasks and operate without strict human control.\n",
    "AI integration also drives natural language and vision-based interaction, letting people command robots using more intuitive inputs.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "Use the following context to answer the question.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\",\n",
    "    input_variables=[\"context\", \"question\"],\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    RunnableLambda(lambda x: x)\n",
    "    | prompt \n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "answer = chain.invoke({\"context\": content, \"question\": \"What are some ways technology has impacted society?\"})\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc11eaaa",
   "metadata": {},
   "source": [
    "# Having multiple Prompts\n",
    "Now that we have a basic understanding of how Prompts work, let's chain multiple prompts and llm responses together to create a more complex chain, and have a class to hold the information.\n",
    "\n",
    "This example we are going to create a review chain that will take in a product review and generate a response based on the sentiment of the review. We will also be able to summarize the review in one sentence.\n",
    "\n",
    "`RunnablePassthrough.assign` will take our input and expand on it to include the summary and sentiment of the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d7bdf7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"author\": \"Arthur\",\n",
      "  \"product\": \"Airfrier\",\n",
      "  \"review\": \"This product is amazing! I've been using it for a few weeks now and I can't get enough of it. It's so easy to use and the customer service is great. I highly recommend it to anyone looking for a reliable and affordable solution.\",\n",
      "  \"summary\": \"The reviewer highly recommends the Airfrier, praising its ease of use, reliability, and affordability, as well as excellent customer service.\",\n",
      "  \"sentiment\": \"Positive\",\n",
      "  \"response\": \"Here's a professional customer service response:\\n\\nDear Arthur,\\n\\nThank you for taking the time to share your wonderful experience with our AirFryer product! We are thrilled to hear that it has exceeded your expectations in terms of ease of use, reliability, and affordability. Your satisfaction is of utmost importance to us, and we're delighted to know that our customer service team has been able to provide you with the support you needed.\\n\\nWe appreciate your kind words about our customer service, and I'm glad to hear that it's met your expectations. Our goal is to ensure that all our customers have a positive experience with our products, and we're committed to providing exceptional service every time.\\n\\nIf there's anything else we can do for you or if you have any further questions or concerns, please don't hesitate to reach out. We value your loyalty and appreciate the opportunity to serve you.\\n\\nThank you again for your review, and we hope you continue to enjoy using your AirFryer!\\n\\nBest regards,\\n[Your Name]\\nCustomer Service Team\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "import json\n",
    "\n",
    "\n",
    "class Review(BaseModel):\n",
    "    author: str = Field(description=\"The author of the review\")\n",
    "    product: str = Field(description=\"The product being reviewed\")\n",
    "    review: str = Field(description=\"The review text\")\n",
    "    summary: str = Field(description=\"The summary of the review\")\n",
    "    sentiment: str = Field(description=\"The sentiment of the review\")\n",
    "    response: str = Field(description=\"The response to the review\")\n",
    "\n",
    "    @staticmethod\n",
    "    def build_review_object(data):\n",
    "        return Review(\n",
    "            author=data[\"author\"],\n",
    "            product=data[\"product\"],\n",
    "            review=data[\"review\"],\n",
    "            summary=data[\"summary\"],\n",
    "            sentiment=data[\"sentiment\"],\n",
    "            response=data[\"response\"],\n",
    "        )\n",
    "\n",
    "\n",
    "class SentimentClassication(BaseModel):\n",
    "    sentiment: str = Field(description=\"The sentiment that the statement is associated with\")\n",
    "\n",
    "\n",
    "sentiment_llm = llm.with_structured_output(SentimentClassication)\n",
    "\n",
    "summarize_prompt = PromptTemplate.from_template(\n",
    "    \"Summarize this {product} review in one sentence:\\n\\n{review}\"\n",
    ")\n",
    "\n",
    "sentiment_prompt = PromptTemplate.from_template(\n",
    "    \"Classify the sentiment of this review as Positive, Negative, or Neutral:\\n\\n{review}\"\n",
    ")\n",
    "response_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Based on this product review summary and sentiment, write a professional customer service response.\n",
    "\n",
    "Author: {author}\n",
    "Product: {product}\n",
    "Summary: {summary}\n",
    "Sentiment: {sentiment}\n",
    "\n",
    "Response:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "review_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        summary=lambda x: (summarize_prompt | llm | StrOutputParser()).invoke(x),\n",
    "        sentiment=lambda x: (sentiment_prompt | sentiment_llm).invoke(x).sentiment,\n",
    "    )\n",
    "    | RunnablePassthrough.assign(response=response_prompt | llm | StrOutputParser())\n",
    "    | RunnableLambda(Review.build_review_object)\n",
    ")\n",
    "\n",
    "\n",
    "review = review_chain.invoke(\n",
    "    {\n",
    "        \"author\": \"Arthur\",\n",
    "        \"product\": \"Airfrier\",\n",
    "        \"review\": \"This product is amazing! I've been using it for a few weeks now and I can't get enough of it. It's so easy to use and the customer service is great. I highly recommend it to anyone looking for a reliable and affordable solution.\",\n",
    "    }\n",
    ")\n",
    "print(json.dumps(dict(review), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ef8fbd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Agentic AI Workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
